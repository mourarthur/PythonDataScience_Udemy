{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Spark e ao Python\n",
    "\n",
    "Vamos aprender a usar Spark com Python usando a biblioteca pyspark! Certifique-se de ter assistido ao vídeo explicando Spark e RDDs antes de continuar neste código.\n",
    "\n",
    "Este notebook servirá como uma referência para a seção Big Data do curso envolvendo o Amazon Web Services. O vídeo fornecerá explicações mais completas sobre o que o código está fazendo.\n",
    "\n",
    "## Criando um SparkContext\n",
    "\n",
    "Primeiro, precisamos criar um SparkContext. Vamos importar isso do pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, crie o SparkContext. Um SparkContext representa a conexão com um cluster Spark e pode ser usado para criar um RDD e variáveis de transmissão nesse cluster.\n",
    "\n",
    "*Nota! Você só pode ter um SparkContext de cada vez da maneira como estamos executando as coisas aqui. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações básicas\n",
    "\n",
    "Vamos começar com um exemplo de \"Olá mundo\", que apenas lerá um arquivo de texto. Primeiro vamos criar um arquivo de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Vamos escrever um exemplo de arquivo de texto para ler usando alguns comandos especiais do notebook jupyter para isso, mas sinta-se livre para usar qualquer arquivo .txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "fourth line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos pegar o arquivo de texto usando o método ** textFile ** do SparkContext que criamos. Este método irá ler um arquivo de texto do HDFS, um sistema de arquivos local (disponível em todos\n",
    "nós), ou qualquer URI do sistema de arquivos compatível com Hadoop, e devolvê-lo como um RDD de Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A abstração primária do Spark é uma coleção distribuída de itens chamados de Conjunto de Dados Distribuido Resiliente (Resilient Distributed Dataset - RDD). Os RDDs podem ser criados a partir de Hadoop InputFormats (como arquivos HDFS) ou transformando outros RDDs.\n",
    "\n",
    "### Ações\n",
    "\n",
    "Acabamos de criar um RDD usando o método textFile e podemos executar operações nesse objeto, como contar as linhas.\n",
    "Os RDDs têm ações, que retornam valores e transformações, que retornam ponteiros para novos RDDs. Vamos começar com algumas ações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações\n",
    "\n",
    "Agora, podemos usar transformações, por exemplo, a transformação filtro retornará um novo RDD com um subconjunto de itens no arquivo. Vamos criar uma transformação de amostra usando o método filter(). Este método (assim como a própria função de filtro do Python) apenas retornará elementos que satisfaçam a condição. Procuremos procurar linhas que contenham a palavra \"segundo\". Nesse caso, deve haver apenas uma linha que tenha isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secfind = textFile.filter(lambda line: 'second' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD\n",
    "secfind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second line']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute a ação na transformação\n",
    "secfind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute a ação na transformação\n",
    "secfind.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como as transformações não exibirão uma saída e não serão executadas até que uma ação seja chamada. No próximo notebook: Spark e Python Avançado, começaremos a ver muitos outros exemplos dessa transformação e relações entre ações!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
